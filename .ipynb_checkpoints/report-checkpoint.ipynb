{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1><center>BBM 409: Introduction to Machine Learning Lab.</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Assignment 3</center></h1>\n",
    "<h1><center>Due on December 20, 2018 (23:59:59)</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Muhammed İkbal Arslan</h3> \n",
    "<h3 align=\"center\">21426611</h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Differences between logistic regression and linear regression\n",
    "\n",
    "If we consider the variable types, the binary logistic regression requires that the dependent variable be binary, while the linear regression needs to be continuous. The Linear regression models data using a continuous numeric value. As against, logistic regression models the data in the binary values. Considering the types of algorithms, linear regression is based on the least square root estimation, indicating that the regression coefficients should be chosen to minimize the sum of the quadratic distances for each observed response. However, Logistic regression is based on Maximum Likelihood Estimation. Linear Regression creates a straight line but Logistic Regression produces a S Curve. In addition, linear regression assumes the normal or Gaussian distribution of a dependent variable. However, Logistic regression assumes the binomial distribution of the dependent variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ logistic = \\frac { 1 } { 1 + \\exp \\left( w _ { 0 } + \\sum _ { i } w _ { i } X _ { i } \\right) }$$\n",
    "$$linear \\space model  =   \\ell ( \\mathbf { w } ) = \\sum _ { n = 1 } ^ { N } \\left[ t ^ { ( n ) } - \\left( w _ { 0 } + w _ { 1 } x ^ { ( n ) } \\right) \\right] ^ { 2 }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Differences between logistic regression and naive bayes methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naïve Bayes conditionally assumes all properties to be independent. Therefore, if some features are actually interdependent, the forecast may be weak. Logistic regression, linearly separates the property field and usually works well, even if some variables are associated.Logistic regression can help reduce overloading when the training data size is small compared to the number of features and may result in a more generalized model. In Naïve Bayes, when the size of the training data is small by the number of features, information about previous possibilities helps improve the results. The learning mechanism is a bit different also between the two models, where Naive Bayes is a generative model and Logistic regression is a discriminative model. Lastly, while Naive Bayes using Generative model, Logistic regression uses Discriminative model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-  Which of the following statements are true?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**False:** A two layer (one input layer, one output layer; no hidden layer) neural network\n",
    "can represent the XOR function. \n",
    "- **Reason:** If we try to draw a line to separate all 1s from all the 0s we can see that this is not possible. We need to draw another line. That’s why we can not use perceptrons (without hidden layer), we will have to use multiple perceptrons to draw multiple lines. XOR is not Linearly Separable. I needs multiple logical operations by using a hidden layer to represent the XOR function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**True:** Any logical function over binary-valued (0 or 1) inputs $x_1$ and $x_2$ can be (approximately)\n",
    "represented using some neural network.\n",
    "- **Reason:** We can (approximately) represent any logical function by composing AND, OR, and NOT functions over multiple layers. Because with using a two layer network, we can build the basic AND, OR, and NOT functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**False:** Suppose you have a multi-class classification problem with three classes, trained with a 3 layer network. Let $a _ { 1 } ^ { ( 3 ) } = \\left( h _ { \\Theta } ( x ) \\right) _ { 1 }$ be the activation of the first output unit and similarly $a _ { 2 } ^ { ( 3 ) } = \\left( h _ { \\Theta } ( x ) \\right) _ { 2 }$ and $ a _ { 3 } ^ { ( 3 ) } = \\left( h _ { \\Theta } ( x ) \\right) _ { 3 }$ . Then for any input x, it must be the case that that $a _ { 1 } ^ { ( 3 ) } + a _ { 2 } ^ { ( 3 ) } + a _ { 3 } ^ { ( 3 ) } = 1 $\n",
    "- **Reason:** If we think any probability problem, we can assume that the sum of probability needs to be 1. But in a neural network, this is not mandatory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**True:** The activation values of the hidden units in a neural network, with the sigmoid activation function applied at every layer, are always in the range (0, 1). \n",
    "- **Reason:** According to the range as logistic regression, this is true.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4- Deciding the number of hidden layers and nodes in a hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Deciding the number of hidden layers and nodes in a hidden layer, we need to use Cross-validation to test the accuracy on the test set. There is no condition for the best number of hidden units. Sometimes It could smaller than the number of inputs. Assume that, we have a lot of training examples and we can use multiple hidden units, but let we think, sometimes fewer data could work best with just 2 hidden units. Using one hidden layer for simple tasks in general, but nowadays research in deep neural network architectures show that many hidden layers can be efficient for the difficult topic, handwritten character, and face recognition problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART II: Classification of Flowers using Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network is trained over these images and then used to predict the label of a given image. Neural network consists of one input layer, one output layer and multiple hidden layers. In determining whether the flowers are belong to which class, I will try to explain my steps in this report. In the first part, I will talk about the process of reading the train, validation and test files and talk about the normalizing, feed forwarding and backprobagation processes that I have followed to increase the accuracy for the model and make the program more accurate. \n",
    "\n",
    "The most important thing is about understanding the image vectors. I will try to determine weights, biases, learning rate, epoch size, batch size, number of hidden layer and unit number of hidden layers that may be useful for classifiying, together with statistics on how often they classifies flowers correctly. \n",
    "\n",
    "The first section of second part is single layer neural network. In this part, I train the network with specific number of parameters and the vectors that is produced from 3000 images. While creating single layer neural network, we have no hidden layer so, one weight matrix and one bias matrix is enough for our network but, while creating multi layer neural network, we need much more than that. Main mentality of neural network is reducing the error loss value that is calculated from each feed forwarding step while updating weight matrices and biases. After model is trained with the hiper parameters, I save the trained parameters and model with pickle to use while testing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Used Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import log10\n",
    "from read_file import read_mat,expectedOutputs,normalize\n",
    "from neural_network import Neural_Network\n",
    "import pickle\n",
    "from scipy.io import loadmat, whosmat\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These libraries used for creating numpy arrays, parsing .mat files into the lists. Additionally, pickle library is used for saving models with optimum parameters. Pickle is very useful for neural network problems in machine learning. Because trained models must be saved with the some tools for usage on testing process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**--**  *Read given train, validation and test datasets and split them according to colon names such as input and expected output.*<br>\n",
    "**--**  *Prepare dataset for flower classification.*<br>\n",
    "**--**  *Normalize the train data with the maximum element of it. In our scenario, normalized parameter is 255.*<br>\n",
    "**--**  *Turn the expected output types to 0 or 1. For example, if expected class is 2, expected outputs is [0,1,0,0,0].*<br>\n",
    "**--**  *Imlement Neural Network model dynamically for each network type (single layer, multi layer).*<br>\n",
    "**--**  *Imlement synapsises for the network. Synapsis in neural network is weights. I keep weights for each layer as matrix type. Initial weight values are produced from negative and positive numbers.*<br>\n",
    "**--**  *Create bias terms for balancing the output nodes of each layer. Bias terms are kept in matrix too.*<br>\n",
    "**--**  *Implement Feed Forward and calculate cross entropy loss for each epoch.*<br>\n",
    "**--**  *Implement Backpropagation and update weights and biases with the derivative of activation function.*<br>\n",
    "**--**  *While running the train function inside the epoch loop, detect the best validation accuracy and train the model with this best fit epoch and learned parameters.*<br>\n",
    "**--**  *Save model with the trained paramaters as .pkl file.*<br>\n",
    "**--**  *Calculate the test accuracy with the trained model for each Neural Network type.*<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Single Layer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Algorithm and My Aproaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our neural network implementation makes predictions using forward propagation, which is the application of the activation function on input from the previous layer until it reaches the last layer as the name output. The neural network is updated mainly in two steps as forward propagation(feed-forwarding) and backward propagation. \n",
    "\n",
    "While doing forward propagation, the input $x_{1}$.....$x_{j}$ matrix is multiplied with the weight matrix that includes the weight of current layer ($w_{ij}$) to get zLast value for each layer and bias[i] is added to the zLast and then sigmoid activation function is applied to get the output of that neuron. Like this process, we keep forward propagating until we reach the last layer, names as output layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$E = - \\sum c i . \\log ( p i ) + ( 1 - c i ) \\cdot \\log ( 1 - p i )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activation functions used for the neural network is sigmoid function for all layers except the output layer. In the output layer I used Softmax function according the assignment. These functions are used because they are both readily differentiable and thus are suited for back propagation. Softmax function is commonly used for multinomial classification problems. The hypothesis $h_{θ} (x)$, representing a softmax function is the probability that Y = k on input X. Thus, predicted outputs estimate probabilities and total sum of estimated probabilities is one from predicted outputs.\n",
    "\n",
    "I used the sigmoid activation function and this is logistic function. Generally, sigmoid is used for the neural networks because it gives flatten results. But sometimes sigmoid stucks during training, because of the fact that if a strongly-negative input is provided to the logistic sigmoid, output values of sigmoid function very near to zero. Since neural networks use the feed forward to find gradients and update model parameters, this results in their updating less regularly than expected and maybe their current state do not change effectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Sigmoid\\space function\\space is : y = \\frac { 1 } { 1 + e ^ { - x } }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Derivative\\space of\\space sigmoid\\space is: \\frac { d y } { d x } = y ( 1 - y )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Derivative\\space of\\space cross\\space entropy\\space result\\space with\\space the\\space softmax\\space function\\space is:\\space expected\\space output\\space -\\space predicted\\space output $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.listendata.com/2014/11/difference-between-linear-regression.html\n",
    "- https://medium.com/@sangha_deb/naive-bayes-vs-logistic-regression-a319b07a5d4c\n",
    "- http://dataespresso.com/en/2017/10/24/comparison-between-naive-bayes-and-logistic-regression/\n",
    "- https://www.quora.com/What-is-XOR-problem-in-neural-networks\n",
    "- https://www.researchgate.net/post/How_to_decide_the_number_of_hidden_layers_and_nodes_in_a_hidden_layer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
