{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Differences between logistic regression and linear regression\n",
    "\n",
    "If we consider the variable types, the binary logistic regression requires that the dependent variable be binary, while the linear regression needs to be continuous. The Linear regression models data using a continuous numeric value. As against, logistic regression models the data in the binary values. Considering the types of algorithms, linear regression is based on the least square root estimation, indicating that the regression coefficients should be chosen to minimize the sum of the quadratic distances for each observed response. However, Logistic regression is based on Maximum Likelihood Estimation. Linear Regression creates a straight line but Logistic Regression produces a S Curve. In addition, linear regression assumes the normal or Gaussian distribution of a dependent variable. However, Logistic regression assumes the binomial distribution of the dependent variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ logistic = \\frac { 1 } { 1 + \\exp \\left( w _ { 0 } + \\sum _ { i } w _ { i } X _ { i } \\right) }$$\n",
    "$$linear \\space model  =   \\ell ( \\mathbf { w } ) = \\sum _ { n = 1 } ^ { N } \\left[ t ^ { ( n ) } - \\left( w _ { 0 } + w _ { 1 } x ^ { ( n ) } \\right) \\right] ^ { 2 }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Differences between logistic regression and naive bayes methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naïve Bayes conditionally assumes all properties to be independent. Therefore, if some features are actually interdependent, the forecast may be weak. Logistic regression, linearly separates the property field and usually works well, even if some variables are associated.Logistic regression can help reduce overloading when the training data size is small compared to the number of features and may result in a more generalized model. In Naïve Bayes, when the size of the training data is small by the number of features, information about previous possibilities helps improve the results. The learning mechanism is a bit different also between the two models, where Naive Bayes is a generative model and Logistic regression is a discriminative model. Lastly, while Naive Bayes using Generative model, Logistic regression uses Discriminative model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-  Which of the following statements are true?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**False:** A two layer (one input layer, one output layer; no hidden layer) neural network\n",
    "can represent the XOR function. \n",
    "- **Reason:** If we try to draw a line to separate all 1s from all the 0s we can see that this is not possible. We need to draw another line. That’s why we can not use perceptrons (without hidden layer), we will have to use multiple perceptrons to draw multiple lines. XOR is not Linearly Separable. I needs multiple logical operations by using a hidden layer to represent the XOR function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**True:** Any logical function over binary-valued (0 or 1) inputs $x_1$ and $x_2$ can be (approximately)\n",
    "represented using some neural network.\n",
    "- **Reason:** We can (approximately) represent any logical function by composing AND, OR, and NOT functions over multiple layers. Because with using a two layer network, we can build the basic AND, OR, and NOT functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**False:** Suppose you have a multi-class classification problem with three classes, trained with a 3 layer network. Let $a _ { 1 } ^ { ( 3 ) } = \\left( h _ { \\Theta } ( x ) \\right) _ { 1 }$ be the activation of the first output unit and similarly $a _ { 2 } ^ { ( 3 ) } = \\left( h _ { \\Theta } ( x ) \\right) _ { 2 }$ and $ a _ { 3 } ^ { ( 3 ) } = \\left( h _ { \\Theta } ( x ) \\right) _ { 3 }$ . Then for any input x, it must be the case that that $a _ { 1 } ^ { ( 3 ) } + a _ { 2 } ^ { ( 3 ) } + a _ { 3 } ^ { ( 3 ) } = 1 $\n",
    "- **Reason:** If we think any probability problem, we can assume that the sum of probability needs to be 1. But in a neural network, this is not mandatory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**True:** The activation values of the hidden units in a neural network, with the sigmoid activation function applied at every layer, are always in the range (0, 1). \n",
    "- **Reason:** According to the range as logistic regression, this is true.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4- Deciding the number of hidden layers and nodes in a hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Deciding the number of hidden layers and nodes in a hidden layer, we need to use Cross-validation to test the accuracy on the test set. There is no condition for the best number of hidden units. Sometimes It could smaller than the number of inputs. Assume that, we have a lot of training examples and we can use multiple hidden units, but let we think, sometimes fewer data could work best with just 2 hidden units. Using one hidden layer for simple tasks in general, but nowadays research in deep neural network architectures show that many hidden layers can be efficient for the difficult topic, handwritten character, and face recognition problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.listendata.com/2014/11/difference-between-linear-regression.html\n",
    "- https://medium.com/@sangha_deb/naive-bayes-vs-logistic-regression-a319b07a5d4c\n",
    "- http://dataespresso.com/en/2017/10/24/comparison-between-naive-bayes-and-logistic-regression/\n",
    "- https://www.quora.com/What-is-XOR-problem-in-neural-networks\n",
    "- https://www.researchgate.net/post/How_to_decide_the_number_of_hidden_layers_and_nodes_in_a_hidden_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
