{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1><center>BBM 409: Introduction to Machine Learning Lab.</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Assignment 3</center></h1>\n",
    "<h1><center>Due on December 20, 2018 (23:59:59)</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Muhammed İkbal Arslan</h3> \n",
    "<h3 align=\"center\">21426611</h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Differences between logistic regression and linear regression\n",
    "\n",
    "If we consider the variable types, the binary logistic regression requires that the dependent variable be binary, while the linear regression needs to be continuous. The Linear regression models data using a continuous numeric value. As against, logistic regression models the data in the binary values. Considering the types of algorithms, linear regression is based on the least square root estimation, indicating that the regression coefficients should be chosen to minimize the sum of the quadratic distances for each observed response. However, Logistic regression is based on Maximum Likelihood Estimation. Linear Regression creates a straight line but Logistic Regression produces a S Curve. In addition, linear regression assumes the normal or Gaussian distribution of a dependent variable. However, Logistic regression assumes the binomial distribution of the dependent variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ logistic = \\frac { 1 } { 1 + \\exp \\left( w _ { 0 } + \\sum _ { i } w _ { i } X _ { i } \\right) }$$\n",
    "$$linear \\space model  =   \\ell ( \\mathbf { w } ) = \\sum _ { n = 1 } ^ { N } \\left[ t ^ { ( n ) } - \\left( w _ { 0 } + w _ { 1 } x ^ { ( n ) } \\right) \\right] ^ { 2 }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Differences between logistic regression and naive bayes methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naïve Bayes conditionally assumes all properties to be independent. Therefore, if some features are actually interdependent, the forecast may be weak. Logistic regression, linearly separates the property field and usually works well, even if some variables are associated.Logistic regression can help reduce overloading when the training data size is small compared to the number of features and may result in a more generalized model. In Naïve Bayes, when the size of the training data is small by the number of features, information about previous possibilities helps improve the results. The learning mechanism is a bit different also between the two models, where Naive Bayes is a generative model and Logistic regression is a discriminative model. Lastly, while Naive Bayes using Generative model, Logistic regression uses Discriminative model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-  Which of the following statements are true?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**False:** A two layer (one input layer, one output layer; no hidden layer) neural network\n",
    "can represent the XOR function. \n",
    "- **Reason:** If we try to draw a line to separate all 1s from all the 0s we can see that this is not possible. We need to draw another line. That’s why we can not use perceptrons (without hidden layer), we will have to use multiple perceptrons to draw multiple lines. XOR is not Linearly Separable. I needs multiple logical operations by using a hidden layer to represent the XOR function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**True:** Any logical function over binary-valued (0 or 1) inputs $x_1$ and $x_2$ can be (approximately)\n",
    "represented using some neural network.\n",
    "- **Reason:** We can (approximately) represent any logical function by composing AND, OR, and NOT functions over multiple layers. Because with using a two layer network, we can build the basic AND, OR, and NOT functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**False:** Suppose you have a multi-class classification problem with three classes, trained with a 3 layer network. Let $a _ { 1 } ^ { ( 3 ) } = \\left( h _ { \\Theta } ( x ) \\right) _ { 1 }$ be the activation of the first output unit and similarly $a _ { 2 } ^ { ( 3 ) } = \\left( h _ { \\Theta } ( x ) \\right) _ { 2 }$ and $ a _ { 3 } ^ { ( 3 ) } = \\left( h _ { \\Theta } ( x ) \\right) _ { 3 }$ . Then for any input x, it must be the case that that $a _ { 1 } ^ { ( 3 ) } + a _ { 2 } ^ { ( 3 ) } + a _ { 3 } ^ { ( 3 ) } = 1 $\n",
    "- **Reason:** If we think any probability problem, we can assume that the sum of probability needs to be 1. But in a neural network, this is not mandatory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**True:** The activation values of the hidden units in a neural network, with the sigmoid activation function applied at every layer, are always in the range (0, 1). \n",
    "- **Reason:** According to the range as logistic regression, this is true.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4- Deciding the number of hidden layers and nodes in a hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Deciding the number of hidden layers and nodes in a hidden layer, we need to use Cross-validation to test the accuracy on the test set. There is no condition for the best number of hidden units. Sometimes It could smaller than the number of inputs. Assume that, we have a lot of training examples and we can use multiple hidden units, but let we think, sometimes fewer data could work best with just 2 hidden units. Using one hidden layer for simple tasks in general, but nowadays research in deep neural network architectures show that many hidden layers can be efficient for the difficult topic, handwritten character, and face recognition problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART II: Classification of Flowers using Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network is a system of software patterned after the operation of neurons in the human brain. Neural networks also called artificial neural networks and ANN is a variety of deep learning technology. In my work, neural network is trained over flower images and then used to predict the label of a given image. Neural network consists of one input layer, one output layer and multiple hidden layers. In determining whether the flowers are belong to which class, I will try to explain my steps in this report. \n",
    "\n",
    "In the first part, I will talk about the process of reading the train, validation and test files and talk about the normalizing, feed forwarding and backprobagation processes that I have followed to increase the accuracy for the model and make the program more accurate. \n",
    "\n",
    "The most important thing is about understanding the image vectors. I will try to determine weights, biases, learning rate, epoch size, batch size, number of hidden layer and unit number of hidden layers that may be useful for classifiying, together with statistics on how often they classifies flowers correctly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Used Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import log10\n",
    "from read_file import read_mat,expectedOutputs,normalize\n",
    "from neural_network import Neural_Network\n",
    "import pickle\n",
    "from scipy.io import loadmat, whosmat\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These libraries used for creating numpy arrays, parsing .mat files into the lists. Additionally, pickle library is used for saving models with optimum parameters. Pickle is very useful for neural network problems in machine learning. Because trained models must be saved with the some tools for usage on testing process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**--**  *Read given train, validation and test datasets and split them according to colon names such as input and expected output.*<br>\n",
    "**--**  *Prepare dataset for flower classification.*<br>\n",
    "**--**  *Normalize the train data with the maximum element of it. In our scenario, normalized parameter is 255.*<br>\n",
    "**--**  *Turn the expected output types to 0 or 1. For example, if expected class is 2, expected outputs is [0,1,0,0,0].*<br>\n",
    "**--**  *Imlement Neural Network model dynamically for each network type (single layer, multi layer).*<br>\n",
    "**--**  *Imlement synapsises for the network. Synapsis in neural network is weights. I keep weights for each layer as matrix type. Initial weight values are produced from negative and positive numbers.*<br>\n",
    "**--**  *Create bias terms for balancing the output nodes of each layer. Bias terms are kept in matrix too.*<br>\n",
    "**--**  *Implement Feed Forward and calculate cross entropy loss for each epoch.*<br>\n",
    "**--**  *Implement Backpropagation and update weights and biases with the derivative of activation function.*<br>\n",
    "**--**  *While running the train function inside the epoch loop, detect the best validation accuracy and train the model with this best fit epoch and learned parameters.*<br>\n",
    "**--**  *Save model with the trained paramaters as .pkl file.*<br>\n",
    "**--**  *Calculate the test accuracy with the trained model for each Neural Network type.*<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization of Image Inputs and Expected Output Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I first normalised the input data consisting of pixel values ranging from 0-255 by dividing all image vector values to 255 which is the max value of pixels. For accurate train learning, this normalizing is necessary and helps me while output normalization for making classification vectors as 0 or 1 within the range 0-5. Main mentality of this process is taking the same level of all image features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Algorithm and My Aproaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My neural network implementation makes predictions using forward propagation, which is the application of the activation function on input from the previous layer until it reaches the last layer as the name output. The neural network is updated mainly in two steps as forward propagation(feed-forwarding) and backward propagation. \n",
    "\n",
    "While doing forward propagation, the input $x_{1}$.....$x_{j}$ matrix is multiplied with the weight matrix that includes the weight of current layer ($w_{ij}$) to get zLast value for each layer and bias[i] is added to the zLast and then sigmoid activation function is applied to get the output of that neuron. Like this process, we keep forward propagating until we reach the last layer, names as output layer.\n",
    "\n",
    "Gradient descent algorithm should be applied on backprob so after I did this, I  calculate the gradients while processing the backward propagation. The derivative of each layer outputs is given by the dot product of the previous error with the derivative of activation function and the input source.\n",
    "\n",
    "While determining the learning rate, I paid attention about cross entropy loss graphs. For the best fit model, I think that the slant of the graph should be stable. Excess fluctuation in graph is not wanted. Additionally, whether single or multilayer neural network, I determine most seated batch size as 20. Because, while batch size increasing within the range of 16 to 128, I saw fluctuations and I thought that this batch improves the algorithm when at the low values. I would like to describe other parts of the algorithm and my views on hyper parameters in the relevant chapters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden Layers Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While training a model, the number and size of the hidden layers are the most important parameters I think. If we determine a large number of layers over 3, the accuracy results would be suffered and this to cause overfitting.  If we choose a less number of layers, this time the exacts opposite happens and caused underfitting. I started with the single layer and at this situation, we have just one weight matrix because there is no hidden layer. Matrix size is 768x5 and was initialized from random numbers. \n",
    "\n",
    "After the single layer neural network, I implement the multilayer neural network. Actually, because the code is dynamic, I just added one hidden layer with the node size 100. According to an article I read, the best fit node size is the square root of multiply results with the input size and output size. Afterward, I determine node size nearly to that number. I did the same thing with the 2 hidden layers neural network  and node size still the same, 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different activation functions are tried for neural network. Meanwhile processing with RELU like step function has no useful derivative (its derivative is 0 everywhere or undefined at the 0 point on x-axis). It doesn’t work for backpropagation, so I gave up about RELU for this reason. \n",
    "\n",
    "The activation functions used for the neural network is sigmoid for all layers except the output layer. In the output layer I used Softmax function according the assignment. These functions are used because they are both readily differentiable and thus are suited for back propagation. Softmax function is commonly used for multinomial classification problems. The hypothesis $h_{θ} (x)$, representing a softmax function is the probability that Y = k on input X. Thus, predicted outputs estimate probabilities and total sum of estimated probabilities is one from predicted outputs.\n",
    "\n",
    "I used the sigmoid activation function and this is a logistic function. Generally, sigmoid is used for the neural networks because it gives flatten results. But sometimes sigmoid stucks during training, because of the fact that if a strongly-negative input is provided to the logistic sigmoid, output values of sigmoid function very near to zero. Since neural networks use the feed forward to find gradients and update model parameters, this results in their updating less regularly than expected and maybe their current state do not change effectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Sigmoid\\space function\\space is : y = \\frac { 1 } { 1 + e ^ { - x } }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Derivative\\space of\\space sigmoid\\space is: \\frac { d y } { d x } = y ( 1 - y )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ RELU\\space function \\space is\\space:\\space g ( z ) = \\max \\{ 0 , z \\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image input gives as the initial information about the calculation with the initial weights. While propagating the neural network iteratively one hidden unit to the another, at each layer I make complex calculations. The first thing to do is making dot product of input and weight matrices. <br>\n",
    "\n",
    "$$ Z = W{^T}X + B $$\n",
    "\n",
    "After that sigmoid function is implemented on it as an activation. At each layer, this process produces the output and this output is used for input from another layer outside the last layer. The architecture of the network is about determining depth and width of the layer parts. Depth is the number of hidden layers. Width is the number of units (nodes) on each hidden layer. There are a few sets of activation functions such as sigmoid, rectified linear unit(RELU) and tangent but as a result, I got the best result from sigmoid. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After I did the feed forward, according to the calculated cross entropy loss I started the reverse train of the model with the backward operation through the network in order to compute the gradients. \n",
    "\n",
    "Therefore, iterating over the nodes with starting at the final node as an input, in reverse order, while computing the derivatives after each step, we give the output of each function as input to the derivative of the same function. This process continues until the epoch loop is finished and allows the neural network model to be trained with feed forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Derivative\\space of\\space cross\\space entropy\\space result\\space with\\space the\\space softmax\\space function\\space is:\\space expected\\space output\\space -\\space predicted\\space output $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Derivative\\space of\\space sigmoid\\space is: \\frac { d y } { d x } = y ( 1 - y )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax and Derivation of Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax function takes an N-dimensional vector of real numbers and flats it into a vector of real number probabilities in range (0,1) which add up to 1. \n",
    "\n",
    "$$p _ { i } = \\frac { e ^ { a _ { i } } } { \\sum _ { k = 1 } ^ { N } e _ { k } ^ { a } }$$\n",
    "\n",
    "Softmax function converts max function to soft type and eliminating the obligation to select one maximum value, it partitions 1 into the outputs with maximal output takes the largest probability of the distribution and the others take smaller probabilities.\n",
    "\n",
    "Due to the specificity of the softmax function, which I mentioned earlier, I use it as a activation function of final layer in neural networks. For this, we need to calculate the derivative or gradient and move it back to the previous layer during back propagation. I used the derivation as below: \n",
    "\n",
    "$$\\frac { \\partial p _ { i } } { \\partial a _ { j } } = \\left\\{ \\begin{array} { l l } { p _ { i } \\left( 1 - p _ { j } \\right) } & { \\text { if } \\quad i = j } \\\\ { - p _ { j } , p _ { i } } & { \\text { if } \\quad i \\neq j } \\end{array} \\right.$$\n",
    "\n",
    "$$\\delta i j = \\left\\{ \\begin{array} { l l } { 1 } & { \\text { if } } & { i = j } \\\\ { 0 } & { \\text { if } } & { i \\neq j } \\end{array} \\right.$$\n",
    "\n",
    "$$\\frac { \\partial p _ { i } } { \\partial a _ { j } } = p _ { i } \\left( \\delta _ { i j } - p _ { j } \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy Loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross entropy loss calculates the difference between the expected results from the output and the predicted values, and the difference between the two by looking at the model's output distributions. Cross entropy defined as below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$E = - \\sum c i . \\log ( p i ) + ( 1 - c i ) \\cdot \\log ( 1 - p i )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross entropy measure is a computational alternative commonly used in error measurement as well as sum squared error calculation. Activation function of nodes are used when there is a probability distribution of each assumption. Therefore, it is used as a loss function in neural networks with softmax activations in the output layer. While I am implementing the cross entropy there was a possibility that a zero or negative number would be included in the logarithmic values, and I often encountered this error. In such a case, my own assumption was to equate the result of that logarithm value to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Single Layer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After explaining all the concepts and assumptions related to my neural network application, the sequence came to second part. The first section of the second part is single layer neural network. In this part, I train the network with specific number of parameters and the vectors that is produced from 3000 images. While creating single layer neural network, we have no hidden layer so, one weight matrix and one bias matrix is enough for our network but, while creating multi layer neural network, we need much more than that. Main mentality of neural network is reducing the error loss value that is calculated from each feed forwarding step while updating weight matrices and biases. After model is trained with the hiper parameters, I saved the trained parameters and models with pickle for using while test process. \n",
    "\n",
    "If you want to train the model with single layer neural network, all you have to do is equalize the number of hidden layers to zero which can be changed dynamically. After the huge trial and error steps, I catched up below optimum hiper parameters with the graphs. See below : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td><img src='Optimum_loss_graph_of_Single_Layer.png'></td><td><img src='Optimum_Train_and_Validation_Accuracy_with_Single_Layer.png'></td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, I got the best loss values with the graph of the Single Layer Neural Network using the learning rate = 0.005 and batch size = 20 while looping till the 300 epoch. Best accuracy results comes from training and validation parts with the learning rate = 0.005, batch size = 20 and 100 epoch as you can see the right of the page above. Hit count of the test data is 211 over 722 and the accuracy is 29.24% at the 63th epoch (according to the highest validation score).\n",
    "\n",
    "However, I would like to explain the 3 graphs below that I took by changing the parameters as well as the optimum value graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td><img src='single_loss_graph_with_0.005_learning_rate.png'></td><td><img src='single_loss_graph_with_0.01_learning_rate.png'></td><td><img src='single_loss_graph_with_0.02_learning_rate.png'></td></tr></table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graphs you have seen above are 3 specific loss graphs of the learning rate between 0.005 and 0.02. As you can see in these 3 graphs that I draw in the name of telling the general belief, as the learning rate increases, the fluctuations on the loss graph during the learning of the model have increased. The reason for this is that it takes some time for some overfit learning values to become stable after sudden changes in weight values. As a result, it should not be forgotten that the shortcuts are not always meant that the reaching to the goal fastly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Multi Layer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With One Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I will explain about the sections starting from the neural network with 1 hidden layer and only the sections that are different from the first part. Because I refrain from repeating the same things. \n",
    "\n",
    "As I mentioned before in the hidden layer topic above, there are some metodologies on determining node size of hidden layer. One of them was to take the square root of the multiply result of inputs and the number of outputs. this result is taken as the number of nodes. As a result of my observations, I increased this value a little more and decided that the ideal number was 100 for me. There are 2 graph below for your understanding of optimum accuracy and loss values with the validations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td><img src='Optimum_loss_graph_of_1_Hidden_Layer.png'></td><td><img src='Optimum_Train_and_Validation_Accuracy_with_1_Hidden_Layer.png'></td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, I got the best loss values with the graph of the Multi Layer Neural Network with 1 hidden layer using the learning rate = 0.005 and batch size = 20 while looping till the 70 epoch. Best accuracy results comes from training and validation parts with the learning rate = 0.005, batch size = 20 and 100 epoch as you can see the right of the page above. Hit count of the test data is 220 over 722 and the accuracy is 30.47% at the 46th epoch (according to the highest validation score with the well train score).\n",
    "\n",
    "Additionally, I would like to explain the 3 graphs below that I took by changing the parameters as well as the optimum value graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td><img src='1_hidden_loss_graph_with_0.005_learning_rate.png'></td><td><img src='1_hidden_loss_graph_with_0.01_learning_rate.png'></td><td><img \n",
    "src='1_hidden_loss_graph_with_0.02_learning_rate.png'></td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graphs you have seen above are 3 specific loss graphs of the learning rate between 0.005 and 0.02. As the learning rate increases, the fluctuations on the loss graph during the learning of the model did not change much. But it is not to change that learning values causes the overfitting on training data with the sudden changes in weight values. Loss values can seems like the normal but accuracy does not increase well with the learning rate 0.01 and 0.02. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Two Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, I will talk about 2 hidden layered neural network. While looping in each epoch with the each batch size of the image parts, learning rate is multiplied with the decay paramater which decreases the loss sequentially for stabilization. The optimum loss and accuracy results are below for your information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td><img src='Optimum_loss_graph_of_2_Hidden_Layer.png'></td><td><img src='Optimum_Train_and_Validation_Accuracy_with_2_Hidden_Layer.png'></td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, I got the best loss values with the graph of the Multi Layer Neural Network with 2 hidden layer using the learning rate = 0.005 and batch size = 20 while looping till the 50 epoch. Best accuracy results comes from training and validation parts with the learning rate = 0.005, batch size = 20 and 100 epoch as you can see the right of the page above. Hit count of the test data is 240 over 722 and the accuracy is 33.24% at the 46th epoch (according to the highest validation score with the well train score).\n",
    "\n",
    "Additionally, I would like to explain the 3 graphs below that I took by changing the parameters as well as the optimum value graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td><img src='2_hidden_loss_graph_with_0.005_learning_rate.png'></td><td><img src='2_hidden_loss_graph_with_0.01_learning_rate.png'></td><td><img \n",
    "src='2_hidden_loss_graph_with_0.02_learning_rate.png'></td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graphs you have seen above are 3 specific loss graphs of the learning rate between 0.005 and 0.02. As the learning rate increases, it is obvious that they are overfit because they perform a sudden landing to near by the zero. Sudden changes on weight values is not normal. While the model will be better trained, sudden weight changes can lead to misclassification of flowers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training set contains of 3000 images and also validation set contains 600 image with the .mat file of image vector for validation process, while training the model with the best fit hyper-parameters. The validation set helped me analyse the performance of my trained model with the untrained images and also helped on considering the over fitting situations.\n",
    "\n",
    "In the light of the parameters I determined during the validation process, the test phase made me see how accurate my model was. Test set contains 722 images as I mentioned before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, I have covered Multilayer Neural Network for flower type classification. While implementing this program, I realized that there are lots of improvements in ANN and ML world that will make our life easier. I have had a lot of experience while creating my model. If I were to speak for this task, I would have been able to better train model and provide greater accuracy if the train data were larger. There were some problems I encountered. But I applied hiper parameters such as learning rate, batch size, hidden layer number and node unit in one hidden layer. Thanks to the experiences I have gained as a result of combinations of these parameters, I tried to determine the best parameters for the optimized model. Additionally, I gained knowledge about the best fit model instead of the overfit or underfit ones. I tried to develop classification abilities of my program while drawing the accuracy and validation plots. The test case did not come as I expected, but I think the reason for this is that the training data is less than the needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running of the program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Someone manage to run my code as below :\n",
    "\n",
    "- python train.py train.mat\n",
    "- python test.py test.mat model.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.listendata.com/2014/11/difference-between-linear-regression.html\n",
    "- https://medium.com/@sangha_deb/naive-bayes-vs-logistic-regression-a319b07a5d4c\n",
    "- http://dataespresso.com/en/2017/10/24/comparison-between-naive-bayes-and-logistic-regression/\n",
    "- https://www.quora.com/What-is-XOR-problem-in-neural-networks\n",
    "- https://www.researchgate.net/post/How_to_decide_the_number_of_hidden_layers_and_nodes_in_a_hidden_layer\n",
    "- https://towardsdatascience.com/multi-layer-neural-networks-with-sigmoid-function-deep-learning-for-rookies-2-bf464f09eb7f\n",
    "- https://stackoverflow.com/questions/3775032/how-to-update-the-bias-in-neural-network-backpropagation\n",
    "- https://www.quora.com/How-can-I-implement-mini-batch-gradient-descent-in-a-neural-network\n",
    "- https://blog.yani.io/deltarule/\n",
    "- https://medium.com/@14prakash/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
